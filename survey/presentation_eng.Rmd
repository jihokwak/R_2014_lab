---
title: "Bayesian Inference using Opinion Survey of Seoul Mayoral Election"
author: "Computer Science Dep. 전희원"
date: '2014-06-23'
output:
  beamer_presentation:
    colortheme: sidebartab
    fig_width: 12
    fonttheme: professionalfonts
    highlight: haddock
    theme: Szeged
---

## Background 

> Bayesian had been used on election result prediction effectively. Most recent prediction was done by `Nate Silver` in the 2012 United States presidential election(he correctly predicted the winner of all 50 states). Formaly, John W.Tukey was projecting the election-day results of presidential contests for national television.


## Purpose 

* Estimate support ratio differences of two candidates using `6.4` Seoul mayoral election.
* Compare result between frequentist and bayesian method.
* Compare `6.4` result with bayesian estimation.


## Data 

* Data : National Election Commission <https://www.nesdc.go.kr>
* Range : `2014-03-24` ~ `2014-05-28`
* `#` of Survey  : 31 
* `#` of Survey Orgs. : 16
* `#` of Request Orgs. : 21


## EDA 


```{r eda,echo=FALSE, message=FALSE, warning=FALSE,dev='png',dpi=600}
library(MCMCpack)
library(lubridate)
library(reshape2)
library(extrafont)
library(ggplot2)


surveys <- read.csv("seoul_survey_2014_U8.csv")



surveys_new <- within(surveys, {
  N   <- 표본크기
  N_p <- 표본크기 * 박원순 /100
  N_j <- 표본크기 * 정몽준 /100
  N_e <- 표본크기 * (100 - 박원순 - 정몽준)/100 
  dt  <- ymd(조사시작)
  })


#EDA 
ggplot(melt(surveys_new, id.vars='dt', measure.vars =c('박원순', '정몽준')), aes(dt, value)) + 
  geom_point(aes(colour=variable)) + 
  stat_smooth(aes(colour=variable), method = "loess") + scale_y_continuous("Support Ratio(%)", limits=c(0,100)) + 
  scale_color_discrete("Candidates", labels=c("Park", "Jung")) + 
  ggtitle("Supports of Two Candidates \n(Loess fitting)") + scale_x_datetime(breaks='7 days') 

```

## Multinomial Likelihood with a Dirichlet Prior

* Support Counts(j :Jung , p :Park, e :Etc.)
    + $n_j, n_p, n_e$
* Likelihood 
    + $X_j,X_p,X_e \sim Multinomial(n, \theta_{n_j}, \theta_{n_p}, \theta_{n_e})$ 
* Prior 
    + $\pi(\theta_j, \theta_p, \theta_e) \propto 1$ 
    + $\theta_{n_j}, \theta_{n_p}, \theta_{n_e} \sim Dirichlet(1,1,1)$ 
* Posterior 
    + $\theta_{n_j}, \theta_{n_p}, \theta_{n_e}|n_j,n_p,n_e \sim Dirichlet(n_j + 1, n_p + 1, n_e + 1)$


## Steps 

1. Set uniform prior 
2. Update posterior distribution parameters on each survey. 
3. Do Monte Carlo Simulation and get samples on each parameters(10,000 samples). 
3. Get $\theta_p - \theta_j$ distribution and mean of that.


## Mean of Posterior

```{r mc, echo=FALSE,message=FALSE, warning=T, dev='png',dpi=600, fig.width=12}
library(ggmcmc, quietly = T)
library(plyr,quietly = T)

surveys_new <- surveys_new[order(surveys_new$dt,decreasing=F),]
surveys_new$N <- with(surveys_new, {N_p + N_j + N_e})


surveys_new_aggr <- ddply(surveys_new, "조사기관", summarize, N_p=sum(N_p), N_j=sum(N_j), N_e=sum(N_e), N=sum(N))




#uniform prior 
alpha <- c(1,1,1)


baye_diffs <- c()
freq_diff <- c()
ci <- c()
#sequential learning  
for(i in 1:nrow(surveys_new)){
  obs <- unlist(surveys_new[i, c("N_p", "N_j", "N_e")])
  post <- MCmultinomdirichlet(obs, alpha, mc=10000)
  baye_diffs <- append(baye_diffs, round(mean(post[,1] - post[,2]), 3))
  alpha <- (alpha + obs)
  
  #samp <- rdirichlet(n, as.vector(alpha))
  #obs[1]/sum(obs) 
  #p <- obs[1]/sum(obs)
  #plus_minus <- qnorm(0.975) * sqrt( (obs[1]/sum(obs) * sum(obs[c(2,3)])/sum(obs))/(sum(obs) - 1))
  #print(sprintf("%f +- %f", p, plus_minus))
  
  p_1 <- obs[1]/sum(obs)
  p_2 <- obs[2]/sum(obs)
  conf_interval <- qnorm(0.975) * 1/sqrt(sum(obs)) * sqrt(p_1 *(1- p_1) + p_2 * (1 - p_2) + 2 * p_1 * p_2)
  freq_diff <- append(freq_diff, p_1 - p_2)
  ci <- append(ci, conf_interval)
}


diff_dist <- data.frame(diffs_val=as.numeric((post[,1] - post[,2])))

mdiff <- mean(diff_dist$diffs_val)

ggplot(diff_dist, aes(diffs_val)) + geom_histogram(binwidth=0.001) + geom_vline(x=get('mdiff',envir =.GlobalEnv)) + 
  scale_x_continuous(breaks=round(c(seq(0.10, 0.15, by=0.01), get('mdiff',envir =.GlobalEnv)),3)) + xlab(expression(mean~of~theta[p]-theta[j])) 

```



## Comparison between Frequentist and Bayesian 

```{r comp, echo=FALSE,message=FALSE, warning=T, dev='png',dpi=600,fig.width=12}
res <- rbind(data.frame(dt=surveys_new$dt, diff_kind="freq", diff_ratio=freq_diff, ci=ci),
      data.frame(dt=surveys_new$dt, diff_kind="bayes", diff_ratio=baye_diffs, ci=0)
      )


Np <- sum(surveys_new$"N_p")
Nj <- sum(surveys_new$"N_j")
sumtotal <- Np + Nj

p_p <- Np/sumtotal
p_j <- Nj/sumtotal

freq_diff_r <- Np/sumtotal - Nj/sumtotal
ci_r <- qnorm(0.975) * 1/sqrt(sumtotal) * sqrt(p_p *(1- p_p) + p_j * (1 - p_j) + 2 * p_p * p_j)
ymax <- freq_diff_r + ci_r
ymin <- freq_diff_r - ci_r

res <- rbind(res, data.frame(dt=ymd('2014-06-01'), diff_kind="freq_total", diff_ratio=freq_diff_r, ci=ci_r))


#melt(res, id.vars='dt', measure.vars=c('freq_diff', 'bayes_diff'))

ggplot(res, aes(x=dt,y=diff_ratio, colour=diff_kind)) + geom_point() + geom_line() + 
  geom_errorbar(aes(ymin=diff_ratio-ci, ymax=diff_ratio+ci), linetype=3, width=0) +  
  geom_hline(yintercept=0.13, linetype=2) + 
  geom_text(data=NULL,x=as.numeric(ymd('2014-04-01')) , y=freq_diff_r, label="6.4 Result", vjust=0, colour='black', size=5) + 
  scale_x_datetime("date",breaks = "7 days") +
  scale_y_continuous( expression(theta[p] - theta[j]), breaks=seq(-0.01,0.25, by=0.01)) + scale_color_discrete("Inference Methods")

```

## Conclusion

* Bayesian results quite good(`r 0.13 - mdiff` error). 
* `0%` error when exclude abnormal survey on `2014-05-28`. 


## References 

* Gelman, et. al. Bayesian Data Analysis 3nd (2013, p. 69)
* Andrew D. Martin, Kevin M. Quinn, Jong Hee Park (2011). MCMCpack: Markov Chain Monte Carlo in R. Journal of
  Statistical Software. 42(9): 1-21. URL http://www.jstatsoft.org/v42/i09/.




